services:
  # ========== MASTER NODE ==========
  namenode:
    build: ./dockerfiles/hadoop-base
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"  # NameNode Web UI
      - "9000:9000"  # HDFS
      - "8088:8088"  # ResourceManager Web UI
      - "19888:19888" # Job History Server
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./config/hadoop:/opt/hadoop/etc/hadoop
      - ./data:/data
      - ./scripts:/scripts
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - YARN_CONF_yarn_resourcemanager_hostname=namenode
      - YARN_CONF_yarn_resourcemanager_bind_host=0.0.0.0
      - YARN_CONF_yarn_nodemanager_bind_host=0.0.0.0
      - YARN_CONF_yarn_timeline_service_bind_host=0.0.0.0
    command: ["/scripts/start-namenode.sh"]
    networks:
      - bigdata-net

  # ========== SECONDARY MASTER ==========
  secondarynamenode:
    build: ./dockerfiles/hadoop-base
    container_name: secondarynamenode
    hostname: secondarynamenode
    ports:
      - "9868:9868"  # Secondary NameNode Web UI
    volumes:
      - hadoop_secondarynamenode:/hadoop/dfs/namesecondary
      - ./config/hadoop:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    command: ["/scripts/start-secondarynamenode.sh"]
    depends_on:
      - namenode
    networks:
      - bigdata-net

  # ========== SPARK MASTER ==========
  spark-master:
    build: ./dockerfiles/spark
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master
      - "4040:4040"  # Spark Application Web UI
    volumes:
      - ./config/spark:/opt/spark/conf
      - ./data:/data
      - ./scripts:/scripts
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    command: ["/scripts/start-spark-master.sh"]
    networks:
      - bigdata-net

  # ========== DATA NODES (SLAVES) ==========
  datanode1:
    build: ./dockerfiles/hadoop-base
    container_name: datanode1
    hostname: datanode1
    ports:
      - "9864:9864"  # DataNode Web UI
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
      - ./config/hadoop:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=namenode
    command: ["/scripts/start-datanode.sh"]
    depends_on:
      - namenode
    networks:
      - bigdata-net

  datanode2:
    build: ./dockerfiles/hadoop-base
    container_name: datanode2
    hostname: datanode2
    ports:
      - "9865:9864"
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
      - ./config/hadoop:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=namenode
    command: ["/scripts/start-datanode.sh"]
    depends_on:
      - namenode
    networks:
      - bigdata-net

  datanode3:
    build: ./dockerfiles/hadoop-base
    container_name: datanode3
    hostname: datanode3
    ports:
      - "9866:9864"
    volumes:
      - hadoop_datanode3:/hadoop/dfs/data
      - ./config/hadoop:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=namenode
    command: ["/scripts/start-datanode.sh"]
    depends_on:
      - namenode
    networks:
      - bigdata-net

  # ========== SPARK WORKERS ==========
  spark-worker1:
    build: ./dockerfiles/spark
    container_name: spark-worker1
    hostname: spark-worker1
    ports:
      - "8081:8081"
    volumes:
      - ./config/spark:/opt/spark/conf
      - ./data:/data
      - ./scripts:/scripts
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    command: ["/scripts/start-spark-worker.sh"]
    depends_on:
      - spark-master
    networks:
      - bigdata-net

  spark-worker2:
    build: ./dockerfiles/spark
    container_name: spark-worker2
    hostname: spark-worker2
    ports:
      - "8082:8081"
    volumes:
      - ./config/spark:/opt/spark/conf
      - ./data:/data
      - ./scripts:/scripts
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    command: ["/scripts/start-spark-worker.sh"]
    depends_on:
      - spark-master
    networks:
      - bigdata-net

  spark-worker3:
    build: ./dockerfiles/spark
    container_name: spark-worker3
    hostname: spark-worker3
    ports:
      - "8083:8081"
    volumes:
      - ./config/spark:/opt/spark/conf
      - ./data:/data
      - ./scripts:/scripts
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    command: ["/scripts/start-spark-worker.sh"]
    depends_on:
      - spark-master
    networks:
      - bigdata-net

  # ========== MONGODB ==========
  mongodb:
    build: ./dockerfiles/mongodb
    container_name: mongodb
    hostname: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
      - ./scripts/mongodb:/docker-entrypoint-initdb.d
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=bigdata123
      - MONGO_INITDB_DATABASE=bigdata
    networks:
      - bigdata-net

  # ========== APPLICATION DYNAMIQUE ==========
  bigdata-app:
    build: ./app
    container_name: bigdata-app
    hostname: bigdata-app
    ports:
      - "3000:3000"
      - "5000:5000"  # API Backend
    volumes:
      - ./app:/app
      - ./data:/data
    environment:
      - NODE_ENV=development
      - MONGODB_URI=mongodb://admin:bigdata123@mongodb:27017/bigdata?authSource=admin
      - HADOOP_NAMENODE=http://namenode:9870
      - SPARK_MASTER=http://spark-master:8080
    depends_on:
      - mongodb
      - namenode
      - spark-master
    networks:
      - bigdata-net

volumes:
  hadoop_namenode:
  hadoop_secondarynamenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_datanode3:
  mongodb_data:

networks:
  bigdata-net:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16